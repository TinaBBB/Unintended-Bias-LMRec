# -*- coding: utf-8 -*-
"""Generate_outputs.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10JqRa9RVa73R6BpMq5T5Lp1lzDH9Vyxo
"""

import argparse
import os
import pickle

import numpy as np
import pandas as pd
import tensorflow as tf

from utils.utils import create_model, get_input_list

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--data_dir', type=str, default='data/Yelp_cities')
    parser.add_argument('--city_name', type=str, default='Austin')
    parser.add_argument('--topk', type=str, default=20, help='top number of recommendations to retrieve')
    parser.add_argument('--test_neutralization', action='store_true', help='whether to perform test-side neutralization towards the results')
    parser.add_argument('--use_tpu', action='store_true', help='whether to use tpu')
    parser.add_argument('--model_dir_root', type=str, default='models/{}/model.h5')
    parser.add_argument('--label_dir_root', type=str, default='data/Yelp_cities/{}_trainValidTest/')
    parser.add_argument('--biasAnalysis_path', type=str, default='data/bias_analysis/yelp/')
    p = parser.parse_args()

    # Get model and label directories
    model_dir = p.model_dir_root.format(p.city_name)
    label_dir = p.label_dir_root.format(p.city_name)
    print('Model directory', model_dir)
    print('Label directory', label_dir)

    # Get input sentence path
    input_path = p.biasAnalysis_path + 'input_sentences/'

    # Set up save path
    if p.test_neutralization:
        save_path = p.biasAnalysis_path + '{}_output_dataframes_Neutralized/'.format(p.city_name)
    else:
        save_path = p.biasAnalysis_path + '{}_output_dataframes/'.format(p.city_name)
    if not os.path.exists(save_path):
        os.mkdir(save_path)
    print('saving to:', save_path)

    # Set maximum len for input text
    if p.city_name == 'Boston':
        max_len = 500
    else:
        max_len = 400

    # load data frame
    city_df = pd.read_csv('{}/{}_reviews.csv'.format(p.data_dir, p.city_name), lineterminator='\n')
    city_df.rename(columns={'stars': 'review_stars', 'text': 'review_text'}, inplace=True)

    df_map = city_df[['business_id', 'name']].drop_duplicates()
    busNumId_2_movieName = df_map.set_index('business_id').T.to_dict('records')[0]

    df_map = city_df[['business_id', 'categories']].drop_duplicates()
    busNumId_2_category = df_map.set_index('business_id').T.to_dict('records')[0]

    # calculate average
    df_map = city_df[['business_id', 'review_stars']].groupby('business_id').mean().reset_index()
    busNumId_2_avgStars = df_map.set_index('business_id').T.to_dict('records')[0]

    df_map = city_df[['business_id', 'price']].drop_duplicates()
    busNumId_2_price = df_map.set_index('business_id').T.to_dict('records')[0]

    """Load model and labels"""
    labels = pickle.load(open(label_dir + "labels.pickle", 'rb'))

    # Create distribution strategy
    if p.use_tpu:
        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
        tf.config.experimental_connect_to_cluster(tpu)
        tf.tpu.experimental.initialize_tpu_system(tpu)
        strategy = tf.distribute.TPUStrategy(tpu)
        with strategy.scope():
            loaded_model = create_model(max_len=max_len, labels=labels)
    else:
        loaded_model = create_model(max_len=max_len, labels=labels)

    loaded_model.load_weights(model_dir)
    print(loaded_model.summary())

    for filename in os.listdir(input_path):
        print(filename)
        if filename.endswith(".csv"):
            current_df = pd.read_csv(input_path + filename)
            current_bais = filename.split('.csv')[0]
            qa_df = pd.DataFrame(columns=current_df.columns)

            pred = loaded_model.predict(get_input_list(current_df['input_sentence'].to_list(), max_len))
            sorted_prediction = np.argsort(-pred)[:, :p.topk]
            restaurant_ids_list = [np.array(labels)[pred] for pred in sorted_prediction]

            for index, row in current_df.iterrows():
                text_input = row['input_sentence']
                for rank, restaurant_id in enumerate(restaurant_ids_list[index]):
                    row["recommended_item"] = busNumId_2_movieName[restaurant_id]
                    row["categories"] = busNumId_2_category[restaurant_id]
                    row["avg_stars"] = round(busNumId_2_avgStars[restaurant_id], 2)
                    row["price"] = busNumId_2_price[restaurant_id]
                    row['rank'] = rank
                    if p.test_neutralization:
                        row['input_sentence'] = text_input

                    qa_df = qa_df.append(row, ignore_index=True)
                if index > 200 and index % 200 == 0:
                    print(index)
                    qa_df.to_csv(save_path + '/yelp_qa_' + current_bais + '.csv')
            qa_df.to_csv(save_path + '/yelp_qa_' + current_bais + '.csv')
        else:
            continue
