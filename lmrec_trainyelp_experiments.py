# -*- coding: utf-8 -*-
"""LMRec_trainYelp_experiments.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-77ZRRRsJqJHfqJ3pmLDpaDTEBhiwyZr

Training the LMRec model using masked data for the Yelp dataset over all the cities
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install tokenizers transformers

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install langdetect

"""# Imports"""

from google.colab import drive
drive.mount('/content/drive')

import re
import pickle
import collections
from langdetect import detect
# from langdetect import detect
import os
import re
import json
import gc
import string
import numpy as np
import pandas as pd

# create IC matrix as CSR matrix
from scipy.sparse import csr_matrix
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tokenizers import BertWordPieceTokenizer
from transformers import BertTokenizer, TFBertModel, BertConfig
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.utils import shuffle
from tensorflow.keras.utils import plot_model
import re
import warnings
warnings.filterwarnings('ignore')

# for nltk related packages
import nltk
nltk.download('stopwords')
nltk.download("vader_lexicon")
nltk.download("averaged_perceptron_tagger")
nltk.download("wordnet")
nltk.download("punkt")

from nltk.corpus import stopwords
stpwrds = set(stopwords.words('english'))
stpwrds.add('by')

# for name entity recognition
import spacy
from spacy import displacy
from collections import Counter
import en_core_web_sm
nlp = en_core_web_sm.load()

configuration = BertConfig()  # default parameters and configuration for BERT

from nltk.sentiment import SentimentAnalyzer
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk.sentiment.util import *
from nltk import tokenize
from nltk.corpus import stopwords
from nltk.tag import PerceptronTagger
from nltk.data import find

"""# Set city domain"""

data_dir = "/content/drive/MyDrive/Yelp_cities"
city_name = 'Portland'
experiment = '5n'
subExp = 'freezeEncoders'

# SET THIS BEFORE EXECUTING
dataset = 'yelp_{}'.format(city_name)

# maximum review length, note that this is set for experiments other than 5f, Boston
if (experiment == '5n'or experiment == '5f') and city_name == 'Boston':
  max_len = 500
else:
  max_len = 400

"""# Functions"""

# plots
def learning_plots(history):
    plt.figure(figsize=(15,4))
    ax1 = plt.subplot(1, 2, 1)
    for l in history.history:
        if l == 'loss' or l == 'val_loss':  
            loss = history.history[l]
            plt.plot(range(1, len(loss) + 1), loss, label=l)

    plt.title('Training and validation loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    ax2 = plt.subplot(1, 2, 2)
    for k in history.history:
        if 'accuracy' in k:  
            loss = history.history[k]
            plt.plot(range(1, len(loss) + 1), loss, label=k)
    plt.title('Training and validation accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()
    plt.savefig('learning.pdf', bbox_inches = 'tight')

# mask-out functions
def multireplace(string, replacements={'/': ',', '(': ',', ')': ''}):
    """
    Given a string and a replacement map, it returns the replaced string.

    :param str string: string to execute replacements on
    :param dict replacements: replacement dictionary {value to find: value to replace}
    :rtype: str

    """
    # Place longer ones first to keep shorter substrings from matching
    # where the longer ones should take place
    # For instance given the replacements {'ab': 'AB', 'abc': 'ABC'} against 
    # the string 'hey abc', it should produce 'hey ABC' and not 'hey ABc'
    substrs = sorted(replacements, key=len, reverse=True)

    # Create a big OR regex that matches any of the substrings to replace
    regexp = re.compile('|'.join(map(re.escape, substrs)))

    # For each match, look up the new string in the replacements
    return regexp.sub(lambda match: replacements[match.group(0)], string)

# convert input text phrase to tokens and attention mask
def get_input(review):
  # Process text
  input_ids = tokenizer.encode(review).ids
  attention_mask = [1] * len(input_ids)
  padding_length = max_len - len(input_ids)
  if padding_length > 0:  # pad
      input_ids = input_ids + ([0] * padding_length)
      attention_mask = attention_mask + ([0] * padding_length)
  else:
      input_ids = input_ids[0:max_len]
      input_ids[-1] = 102   # separation token
      attention_mask = attention_mask[0:max_len]

  return [np.array([input_ids]), np.array([attention_mask])]

def pickle_dump(path, file):
  with open(path, 'wb') as f:
    pickle.dump(file, f)

def pickle_load(path):
  with open(path, 'rb') as fp:
    file = pickle.load(fp)
  return file

# model
def create_model(learning_rate=5e-5, experiment='5f', subExp='freezeBoth',  max_len=400):
    ## BERT encoder
    encoder = TFBertModel.from_pretrained("bert-base-uncased")
    print(experiment)

    if experiment == '5n':
      print('Freezing BERT weights')

      if subExp == 'freezeEmbeddings':
        print('freezing embeddings')
        encoder.bert.embeddings.trainable=False
      elif subExp == 'freezeEncoders':
        print('freezing encoder layers')
        for layer in encoder.bert.encoder.layer:
          layer.trainable = False
      elif subExp == 'freezeBoth':
        print('freezing embeddings')
        encoder.bert.embeddings.trainable=False
        print('freezing encoder layers')
        for layer in encoder.bert.encoder.layer:
          layer.trainable = False
      else:
        assert False, 'Unrecognized sub experiment type'

      # try all freezing - Model not learn
      # encoder.layers[0].trainable = False

      # freezing embedding weights,, not working
      # print('freezing embeddings')
      # encoder.bert.embeddings.trainable=False

      # freezing encoder layers
      # print('freezing encoder layers')
      # for layer in encoder.bert.encoder.layer:
      #   layer.trainable = False

    ## Model
    input_ids = layers.Input(shape=(max_len,), dtype=tf.int32)
    attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32)
    
    embedding = encoder(
        input_ids, attention_mask=attention_mask
    )['pooler_output']

    dense = layers.Dense(1024, activation='relu')(embedding)
    out = layers.Dense(len(labels), activation='softmax')(dense)

    model = keras.Model(
        inputs=[input_ids, attention_mask],
        outputs=out,)
    
    loss = keras.losses.SparseCategoricalCrossentropy()
    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])
    return model

# Save the slow pretrained tokenizer
slow_tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
save_path = "bert_base_uncased/"
if not os.path.exists(save_path):
    os.makedirs(save_path)
slow_tokenizer.save_pretrained(save_path)

# Load the fast tokenizer from saved file
tokenizer = BertWordPieceTokenizer("bert_base_uncased/vocab.txt", lowercase=True)

"""# (Optional) Create Train Validation Test data

## Load Datasets
"""

# saving directory for the train, validation, test data
save_dir = '/content/drive/MyDrive/Yelp_cities/{}_trainValidTest_{}/'.format(city_name, experiment)
if not os.path.exists(save_dir):
  os.mkdir(save_dir)

# load labels directory
if experiment == '5n':
  load_dir = '/content/drive/MyDrive/Yelp_cities/{}_trainValidTest_5f/'.format(city_name)
else:
  load_dir = '/content/drive/MyDrive/Yelp_cities/{}_trainValidTest_{}/'.format(city_name, experiment)

# load data frame
city_df = pd.read_csv('{}/{}_reviews.csv'.format(data_dir, city_name), lineterminator='\n')
city_df.rename(columns={'text': 'review_text'}, inplace=True)

# load labels
labels = pickle_load(load_dir + 'labels.pickle'.format(city_name))

city_df = city_df[city_df['business_id'].isin(labels)]
print('Number of reviews being used for city {}: {} '.format(city_name, len(city_df)))

df = city_df.copy()

df_shrink = df.copy()
df_shrink = df_shrink[['business_id', 'categories']]
lst = df_shrink.categories.values.tolist()
cat = []
replacements = {'&':',', 
                '/': ',',
                '(': ',',
                ')': ''}
for i in range(len(lst)):
    text = multireplace(lst[i], replacements)
    cat.extend(text.lower().split(', '))

Category_set = set(cat)

# get item names
item_names_orig = df.name.unique()
item_names = list()
name_replacements = {' & ': '',
                     '\'s': '',
                     ' - ':''}
for iName in item_names_orig:
  split_name = multireplace(iName.lower(), name_replacements).strip(""" ,.*()[]!@#$%^&*{}?'`"-""").split(" ")
  item_names.append([wrd for wrd in split_name if wrd not in stpwrds])

df = df[['business_id', 'review_text']]
df.columns = ["item", "review_text"]
df.head()
data = df.values.tolist()

# city_df[['review_stars', 'review_text', 'user_id', 'name', 'categories']].sample(frac=1).head(10)

"""### Extract synthetic data from reviews"""

# # create sentiment analyzer
# new_words = pickle_load('/content/drive/MyDrive/Research_material/Research_data/Yelp_cities/sentimentNewWordsDict.pickle')
# sid = SentimentIntensityAnalyzer()

# #Update new words with polarity score
# sid.lexicon.update(new_words)

# Noun Phrase Extraction Support Functions
lemmatizer = nltk.WordNetLemmatizer()
stemmer = nltk.stem.porter.PorterStemmer()

"""### Helper Functions for extracting reviews"""

# Use vader to evaluate sentiment of reviews, vader here!
def evalSentences(sentences, to_df=False, columns=[]):
    #Instantiate an instance to access SentimentIntensityAnalyzer class
    if not sentences: 
        return pd.DataFrame({'A' : []})
    else:
        pdlist = []
        if to_df:
            for sentence in sentences:
                ss = sid.polarity_scores(sentence)
                pdlist.append([sentence]+[ss['compound']])
            reviewDf = pd.DataFrame(pdlist)
            reviewDf.columns = columns
            return reviewDf

        else:
            for sentence in sentences:
                print(sentence)
                ss = sid.polarity_scores(sentence)
                for k in sorted(ss):
                    print('{0}: {1}, '.format(k, ss[k]), end='')

# generator, generate leaves one by one
# Filter only on NP (The other two labels may not be useful)
def leaves(tree):
    """Finds NP (nounphrase) leaf nodes of a chunk tree."""
    for subtree in tree.subtrees(filter = lambda t: t.label()=='NP' or t.label()=='JJ' or t.label()=='RB'):
        yield subtree.leaves()

# stemming, lematizing, lower case...
def normalise(word):
    """Normalises words to lowercase and stems and lemmatizes it."""
    word = word.lower()
    if word != 'was':
        word = lemmatizer.lemmatize(word)
    return word
        
# stop-words and length control
def acceptable_word(word):
    """Checks conditions for acceptable word: length, stopword."""
    special_non_stopwords = ['is','was','are','were'] # Kick out a few words from stopwords list
    final_stop_words = list([word for word in stopwords.words('english') if word not in special_non_stopwords])
    accepted = (bool(2 <= len(word) <= 40)
    and word.lower() not in final_stop_words)
    return accepted
        
# generator, create item once a time
def get_terms(tree):
    for leaf in leaves(tree):
        # Normalize word in 
        term = [normalise(w) for w,t in leaf if acceptable_word(w) ]
        if len(term)>=1:
            yield term

# Flatten phrase lists to get tokens for analysis
def flatten(npTokenList):
    finalList =[]
    for phrase in npTokenList:
        token = ''
        for word in phrase:
            token += word + ' '
        finalList.append(token.rstrip())
    return finalList

# noun phrases 
grammar_nn = r"""
    NBAR:
        {<NN|NNS>+}
    NP:
        {<NBAR>}
"""

# proper noun
grammar_pn = r"""
    NBAR:
        {<NNP|NNPS>+}
    NP:
        {<NBAR>}
"""

# extract specific phrases in the given grammar according to experiment
def get_specific_phrase_list(experiment, review_text):
    tagger = PerceptronTagger()
    pos_tag = tagger.tag
    taggedToks = pos_tag([word for word in re.findall(r'[\w]+[-[\w]+]?|[.,!?;]', review_text)])
    # print(taggedToks)

    # Create phrase tree
    if experiment == '4f': # proper nouns only
      chunker_pn = nltk.RegexpParser(grammar_pn)
      tree_pn= chunker_pn.parse(taggedToks)

      # Traverse tree and get noun phrases
      npTokenList1 = [word for word in get_terms(tree_pn)]
      npTokenList2 = []

    if experiment == '3f': # all noun phrases
      chunker_pn = nltk.RegexpParser(grammar_pn)
      tree_pn= chunker_pn.parse(taggedToks)
      chunker_nn = nltk.RegexpParser(grammar_nn)
      tree_nn= chunker_nn.parse(taggedToks)

      # Traverse tree and get noun phrases
      npTokenList1 = [word for word in get_terms(tree_pn)]
      # npTokenList
      npTokenList2 = [word for word in get_terms(tree_nn)]

    # Combine np token list to one
    npTokenList = []
    for i in npTokenList1 + npTokenList2:
        if i not in npTokenList:
            npTokenList.append(i)
    # npTokenList = []
    # for i in npTokenList_duplicate:
    #     if i not in npTokenList:
    #         npTokenList.append(i)
    
    Extracted_list1 = flatten(npTokenList1)
    Extracted_list2 = flatten(npTokenList2)

    Extracted_list = Extracted_list1 + list(set(Extracted_list2) - set(Extracted_list1))
    return Extracted_list, npTokenList

def get_noun_list(npToken_List,stop_words):
    # create a list of NN from the Keyphrase list
    NN_List=[]
    
    for word in npToken_List:
        phrase = pos_tag(word)
        for term in phrase:
            if term[1]=='NN':
                NN_List.append(term[0])
    NN_List=list(dict.fromkeys(NN_List))
    
    # filter out the stop words
    NN_List= [x for x in NN_List if x not in stop_words]
    return NN_List

# Rank the noun words/phrases based on the total number of appearances in all the reviews (if the phrase existed one or more times in a review, count once…)
def get_score_from_freq(NN_List,review_text,df_spe_item):
    lemmatizer = nltk.WordNetLemmatizer()
    stop = set(stopwords.words('english'))
    
    # 1. Get the frequency of the noun from the whole review, store in the dict_Counter
    dict_Counter = {}
    c = collections.Counter([normalise(word) for word in re.findall(r'[\w]+[-[\w]+]?|[.,!?;]', review_text) if word.lower() not in stop and len(word) > 2])
    for term in NN_List:
        if term in c:
            dict_Counter[term] = dict(c)[term]

    
    # 2. Get the binary frequency of the noun for each review. If the word in this review, then it is 1, otherwise is 0
    # store in the review_freq_counter
    # initialize review_freq_counter
    review_freq_counter = dict()
    # for k in dict_Counter.keys():
    #     review_freq_counter[k] = 0
        
    for i in range(len(df_business.review_text.values)):
        temp = lemmatizer.lemmatize(df_business.review_text.values[i]).lower()
        temp_list = re.findall(r'[\w]+[-[\w]+]?|[.,!?;]',temp)
        for word in dict_Counter.keys():
            if word in temp_list:
                review_freq_counter[word] = review_freq_counter.get(word, 0)+1 

    #sort it and store it in a list
    final_score_counter=[]
    for key, value in sorted(review_freq_counter.items(), key=lambda item: item[1], reverse = True):
        temp = [key,value]
        final_score_counter.append(temp)
    return final_score_counter

#Extract the key phrases that contain the top noun phrases 
#(top 3 nouns, so three lists of key phrases will be generated and each contains the top frequency nouns)

def get_keyphrase_contain_topk(top_word_list, k, Extracted_list):
    topk_freq_words = [x[0] for x in top_word_list[0:k]]
    keyphrase_list = []
    for key_word in Extracted_list:
        for freq_word in topk_freq_words:
            if freq_word in key_word:
                keyphrase_list.append(key_word)
    return list(set(keyphrase_list))

# Get the top sentiment scored a key phrase from each of the lists
def get_final_keyphrase(df_keyphrase, topk, top_word_list):
    res_dict = {}
    topk_freq_words = [x[0] for x in top_word_list[0:topk]]
    for word in topk_freq_words:
        for i in range(df_keyphrase.keyphrase_list.values.shape[0]):
            #deduplicate with excisting keyphrases
            if df_keyphrase["keyphrase_list"][i] not in list(res_dict.values()):
                if word in df_keyphrase["keyphrase_list"][i].split():
                    if word not in res_dict:
                        res_dict[word] = df_keyphrase["keyphrase_list"][i]
                    if word in res_dict:
                        if df_keyphrase["Vader"][i] > df_keyphrase[df_keyphrase["keyphrase_list"] == res_dict[word]].Vader.values[0]:
                            res_dict[word] = df_keyphrase["keyphrase_list"][i]
    
    for (key,val) in res_dict.items():
        #replace words
        val = val.replace("was", "is").replace("were","are").replace("also","").replace("still","").replace("actual","")
        #deduplicate
        res_dict[key] = ' '.join(list(dict.fromkeys(val.split())))
        
        # reorder to a list
#     res_list = []
#     for i in topk_freq_words:
#         res_list.append([i,res_dict[i]])
    return res_dict

def get_business_review(business_id,df):
    df_subset = df.loc[df['business_id'] == business_id]
    review_text = df_subset['review_text'].sum()
    return review_text, df_subset

stop_words_defined = ["place","restaurant","food", "amount","experience",
                      "side","size","location","quality" ,"taste",
                      "thing","order","bowl","night","day","flavour",
                      "spot","portion","dish", "service", "meal","good","drink","time","everything","get",'delivery','nice',"customer","one"]
tagger = PerceptronTagger()
pos_tag = tagger.tag
# how many nouns are we looking at
top_k = 3

labels = []
input_ids_list = []
attention_mask_list = []
y = []
raw_reviews = list()
freq_dict = {}

with tqdm(total=len(data)) as pbar:
  for idx, v in enumerate(shuffle(data,random_state=0)):
      pbar.update(1)
      item = v[0]
      review_text = v[1]  

      # before masking
      # print('Before masking : ' +review_text)
      
      # # for balancing the dataset and check if enough review for the item
      # if (balance_data and freq_dict.get(item,0) > min_reviews) or item_dist[item] < min_reviews:
      #   continue
      
      # language detection
      try:
        review_language = detect(re.sub('\W+','', review_text)[:20])
        if review_language == 'zh-cn':
          #print(review_text[:20])
          continue
      except:
        continue
        
      # get all noun phrases
      Extracted_list, npTokenList = get_specific_phrase_list(experiment, review_text)
      # print(npTokenList)

      # Mask out the noun phrases
      for token_txt in npTokenList:
        for token in token_txt:
          review_text = re.compile(re.escape(token), re.IGNORECASE).sub('[MASK]', review_text)
          #print(review_text)
      
      # after masking
      # print('After masking : ' + review_text)
      
      tokenized_review = tokenizer.encode(review_text)

      # name and then review
      raw_reviews.append([item,review_text])
      input_ids = tokenized_review.ids
      attention_mask = [1] * len(input_ids)    
      # Pad and create attention masks.
      # Truncate if needed
      padding_length = max_len - len(input_ids)
      if padding_length > 0:  # pad
          input_ids = input_ids + ([0] * padding_length)
          attention_mask = attention_mask + ([0] * padding_length)
      else:
          input_ids = input_ids[0:max_len]
          input_ids[-1] = 102
          attention_mask = attention_mask[0:max_len]
      input_ids_list.append(input_ids)
      attention_mask_list.append(attention_mask)
      # Process labels 
      if item not in labels:
          labels.append(item)
      y.append(labels.index(item))
      freq_dict[item] = freq_dict.get(item,0)+1

save_dir

"""## <font color='red'> Run this cell at the first time of data generation </font>"""

# data
save_dir = '/content/drive/MyDrive/Thesis Code/Research_data/Yelp_cities/{}_Masked_trainValidTest_{}/'.format(city_name, experiment)
input_ids_list, attention_mask_list, y, raw_reviews = shuffle(input_ids_list, attention_mask_list, y, raw_reviews, random_state=0)
pickle_dump(save_dir + 'input_ids_list.pickle', input_ids_list)
pickle_dump(save_dir + 'attention_mask_list.pickle', attention_mask_list)
pickle_dump(save_dir + 'y.pickle', y)
pickle_dump(save_dir + 'labels.pickle',labels)
pickle_dump(save_dir + 'raw_reviews.pickle',raw_reviews)



"""# Load Train Validation Test data

#### <font color='green'>Run this cell if the data were generated<font color='green'>
"""

# load data
if experiment == '5n':
  load_dir = '/content/drive/MyDrive/Yelp_cities/{}_trainValidTest_5f/'.format(city_name)
else:
  load_dir = '/content/drive/MyDrive/Yelp_cities/{}_trainValidTest_{}/'.format(city_name, experiment)

input_ids_list = pickle_load(load_dir+ 'input_ids_list.pickle'.format(city_name))
attention_mask_list = pickle_load(load_dir + 'attention_mask_list.pickle'.format(city_name))
y = pickle_load(load_dir + 'y.pickle'.format(city_name))
labels = pickle_load(load_dir + 'labels.pickle'.format(city_name))

# Split into train and test
train_size = int(len(y) * 0.8)
eval_size = int(len(y) * 0.1)

X_train = [np.array(input_ids_list[0:train_size]), np.array(attention_mask_list[0:train_size])]
X_eval = [np.array(input_ids_list[train_size:train_size+eval_size]), np.array(attention_mask_list[train_size:train_size+eval_size])]
X_test = [np.array(input_ids_list[train_size+eval_size:]), np.array(attention_mask_list[train_size+eval_size:])]
del input_ids_list, attention_mask_list
gc.collect()

y_train = np.array(y[0:train_size])
y_eval = np.array(y[train_size:train_size+eval_size])
y_test = np.array(y[train_size+eval_size:])
del y
gc.collect()

"""# Create and train model"""

save_path = "/content/drive/MyDrive/TF_TPU/models/{}_{}/".format(dataset, experiment)

if not os.path.exists(save_path):
  os.mkdir(save_path)
print('save path:', save_path)

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# # Create distribution strategy
# tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
# tf.config.experimental_connect_to_cluster(tpu)
# tf.tpu.experimental.initialize_tpu_system(tpu)
# strategy = tf.distribute.TPUStrategy(tpu)

patience = 1
bs = 64

# create callback and checkpoint
earlyStop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=patience)

if experiment == '5n':
  checkpoint_filepath = save_path + 'bs_{}_{}'.format(bs, subExp) + 'model.{epoch:02d}-{val_accuracy:.4f}.h5'
else:
  checkpoint_filepath = save_path + 'bs_{}'.format(bs) + 'model.{epoch:02d}-{val_accuracy:.4f}.h5'
checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_filepath,
    save_weights_only=True,
    monitor='val_accuracy',
    mode='max',
    save_best_only=True)

eps = 30
learning_rate = 5e-5 # 5n i used 1e-4

resume_epoch = 0

# Create model
with strategy.scope():
  model = create_model(learning_rate=learning_rate, experiment=experiment, subExp=subExp, max_len=max_len)
# model.load_weights(save_path + 'bs_64_freezeBothmodel.08-0.1375.h5')

model.summary()
# plot_model(model)

history = model.fit(
    X_train,
    y_train,
    validation_data = (X_eval, y_eval), 
    epochs=eps, 
    verbose=1,
    batch_size=bs,
    callbacks=[earlyStop_callback, checkpoint_callback],
)
learning_plots(history)

del X_train, y_train, X_eval, y_eval
gc.collect()

# get test accuracy
model.compile(loss='sparse_categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

score, acc = model.evaluate(X_test, y_test,
                            batch_size=bs)
print('Test score:', score)
print('Test accuracy:', acc)

"""### Save model and label code below"""

pickle_dump(save_path + 'labels.pickle', labels)

# saving here is not needed since it's already overfitted after the early stop
# model.save_weights('{}model_epoch{}_bs{}_acc{}_{}.h5'.format(save_path, len(history.history['loss']) - patience + resume_epoch, bs, round(acc*100,2), subExp))
# model.save_weights('{}model_epoch{}_bs{}_acc{}_{}.h5'.format(save_path, 21, bs, round(acc*100,2), subExp))

"""# Performance Evaluation

Using the evaluation method provided by Reda's code

Evaluation for all city's data
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install pytrec_eval

# Commented out IPython magic to ensure Python compatibility.
import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt
import pandas as pd
import pytrec_eval
import numpy as np
import seaborn as sns
from tqdm import tqdm
import pickle

# %matplotlib inline
# %load_ext autoreload
# %autoreload 2
# %load_ext tensorboard

def load_trainValidData(load_dir,  city_name, x_only=False, y_only=False):
  input_ids_list = pickle_load(load_dir+ 'input_ids_list.pickle'.format(city_name))
  attention_mask_list = pickle_load(load_dir + 'attention_mask_list.pickle'.format(city_name))
  y = pickle_load(load_dir + 'y.pickle'.format(city_name))

  # Split into train and test
  train_size = int(len(y) * 0.8)
  eval_size = int(len(y) * 0.1)

  if x_only:
    return [np.array(input_ids_list[train_size+eval_size:]), np.array(attention_mask_list[train_size+eval_size:])]
  if y_only:
    return np.array(y[train_size+eval_size:])

  X_test = [np.array(input_ids_list[train_size+eval_size:]), np.array(attention_mask_list[train_size+eval_size:])]
  y_test = np.array(y[train_size+eval_size:])
  del input_ids_list, attention_mask_list, y
  gc.collect()

  return X_test, y_test

def get_business_to_categories(df):
  business_to_categories = {}
  for business_id, categories in zip(df['business_id'], df['categories']):
      categories = multireplace(categories)
      categories = set(categories.lower().split(', '))
      if 'restaurants' in categories:
          categories.remove('restaurants')
      if business_id in business_to_categories:
          categories = categories | business_to_categories[business_id]
      business_to_categories[business_id] = categories
  return business_to_categories

"""## Predictions"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# # Create distribution strategy
# tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
# tf.config.experimental_connect_to_cluster(tpu)
# tf.tpu.experimental.initialize_tpu_system(tpu)
# strategy = tf.distribute.TPUStrategy(tpu)

experiment = 'Synthetic'
subExp = None #'freezeBoth' # freezeEmbeddings, freezeEncoders

# create saving directory
if experiment == '5n':
  temp_result_dir = '/content/drive/MyDrive/TF_TPU/Performance_Analysis/{}/{}/'.format(experiment, subExp)
else:
  temp_result_dir = '/content/drive/MyDrive/TF_TPU/Performance_Analysis/{}/'.format(experiment)
if not os.path.isdir(temp_result_dir):
  os.mkdir(temp_result_dir)

print('Experiment:{}, SubExperiment:{}'.format(experiment, subExp))

# to be changed
# model_mappings = {
#     'Atlanta': 'bs_32model.05-0.2036.h5',
#     'Austin':'bs_32model.07-0.1525.h5',
#     'Boston': 'model.05-0.19.h5',
#     'Columbus': 'model.06-0.19.h5',
#     'Orlando': 'bs_64model.06-0.1701.h5',
#     'Portland': 'bs_32model.05-0.1635.h5',
#     'Toronto': 'bs_32model.05-0.19.h5'
# }

# TODO better to create a dictionary file for this. 

#4f
if experiment == '4f':
  model_mappings = {'Atlanta': 'bs_64model.06-0.3668.h5',
                    'Austin': 'bs_64model.07-0.3627.h5',
                    'Boston': 'bs_64model.08-0.4209.h5',
                    'Columbus':'bs_64model.09-0.3390.h5', 
                    'Orlando': 'bs_64model.07-0.3620.h5',
                    'Portland': 'bs_64model.07-0.3646.h5', # but only 3 epochs?
                    'Toronto':'bs_64model.07-0.3910.h5'
                    }
                    #'Orlando': 'bs_64model.07-0.4200.h5' # from archive?

# 5f
if experiment == '5f':
  model_mappings = {
    'Atlanta': 'model.07-0.50.h5',
    'Austin': 'model.07-0.49.h5',
    'Boston': 'model.05-0.56.h5',
    'Columbus': 'model_min50.05-0.49.h5',
    'Orlando': 'model.07-0.49.h5',
    'Portland': 'model.07-0.49.h5',
    'Toronto': 'model.07-0.52.h5'}
    # 'Cambridge': 'model.05-0.62.h5'


#3f
if experiment == '3f':
  model_mappings = {'Columbus':'bs_64model.07-0.3610.h5', 
                    # 'Toronto':'bs_64model.06-0.3844.h5', 
                    'Orlando':'bs_64model.06-0.3710.h5'}
                    # 'Boston':'bs_64model.05-0.1909.h5'}

# 3.5f? noun phrases called synthetic before
if experiment == 'Synthetic':
  model_mappings = {    
      'Atlanta': 'bs_32model.05-0.2036.h5',
      'Austin':'bs_32model.07-0.1525.h5',
      'Boston': 'bs_64model.05-0.1870.h5',
      'Columbus': 'model.06-0.19.h5',
      'Orlando': 'bs_64model.06-0.1701.h5',
      'Portland': 'bs_32model.05-0.1635.h5',
      'Toronto': 'bs_32model.05-0.19.h5'}

# 5n
if experiment == '5n':
  if subExp == 'freezeBoth':
    model_mappings = {'Atlanta_freezeBoth': 'bs_64_freezeBothmodel.14-0.1405.h5',
                      'Austin_freezeBoth': 'bs_64_freezeBothmodel.15-0.1333.h5',
                      'Boston_freezeBoth' : 'bs_64_freezeBothmodel.21-0.1933.h5',
                      'Columbus_freezeBoth': 'model_epoch21_bs64_acc14.75_freezeBoth.h5',
                      'Orlando_freezeBoth' : 'bs_64_freezeBothmodel.18-0.1611.h5',
                      'Portland_freezeBoth': 'bs_64_freezeBothmodel.15-0.1451.h5', 
                      'Toronto_freezeBoth': 'model_epoch9_bs64_acc13.3_freezeBoth.h5',
                      }
  elif subExp == 'freezeEmbeddings':
    model_mappings = {
                      'Toronto_freezeEmbeddings': 'model_epoch7_bs64_acc52.42_freezeEmbeddings.h5',
                      }
  
  elif subExp == 'freezeEncoders': # TODO, get improved results
    model_mappings = {
                  'Columbus_freezeEncoders': 'bs_64_freezeEncodersmodel.13-0.4333.h5',
                  'Toronto_freezeEncoders': 'model_epoch9_bs64_acc44.52_freezeEncoders.h5',
                  'Orlando_freezeEncoders': 'bs_64_freezeEncodersmodel.13-0.4449.h5'
                  }
  else:
    assert False, "Unrecognized sub experiment"

#6f
if experiment == '6f':
  model_mappings = {'Columbus':'model.01-0.01.h5'}

"""### Gather prediction data

#### 1 Save labels dict
"""

# 1. save labels dict
labels_dict = {}
for city, model_file_name in model_mappings.items():
    print(city)
    city_origin = city
    if '_' in city:
      city, _ = city.split('_')
    if experiment == '5n' or experiment == 'Synthetic':
      label_dir = '/content/drive/MyDrive/Yelp_cities/{}_trainValidTest_5f/'.format(city)
    else:
      label_dir = '/content/drive/MyDrive/Yelp_cities/{}_trainValidTest_{}/'.format(city, experiment)
    labels = pickle_load(label_dir + 'labels.pickle'.format(city))
    labels_dict[city_origin] = labels.copy()
    del labels
    gc.collect()

pickle_dump('{}labels_dict.pickle'.format(temp_result_dir), labels_dict)

del labels_dict
gc.collect()

"""#### 2 save ground truth dict"""

# 2. save ground truth dict
ground_truth = {}
for city, model_file_name in model_mappings.items():
    print(city)
    city_origin = city
    if '_' in city:
      city, _ = city.split('_')
    dataset = 'yelp_{}'.format(city)
    if experiment == '5n':
      label_dir = '/content/drive/MyDrive/Yelp_cities/{}_trainValidTest_5f/'.format(city)
    else:
      label_dir = '/content/drive/MyDrive/Yelp_cities/{}_trainValidTest_{}/'.format(city, experiment)

    # load testing data
    try:
      y_test = load_trainValidData(label_dir, city, y_only=True)
      ground_truth[city_origin] = y_test.copy()
      del y_test
      gc.collect()
    except:
      print('skipping:', city)
      continue

    

pickle_dump('{}ground_truth.pickle'.format(temp_result_dir), ground_truth)

del ground_truth
gc.collect()

"""#### 3 save predictions dict"""

# 3. save predictions dict
predictions = {}
for city, model_file_name in model_mappings.items():
    city_origin = city
    if '_' in city:
      city, _ = city.split('_')

    if (experiment == '5f' or experiment == '5n') and city =='Boston':
      max_len = 500
    else:
      max_len = 400
    print(city)
    
    dataset = 'yelp_{}'.format(city)
    load_dir = "/content/drive/MyDrive/TF_TPU/models/{}_{}/".format(dataset, experiment)
    if experiment == '5n':
      label_dir = '/content/drive/MyDrive/Yelp_cities/{}_trainValidTest_5f/'.format(city)
    else:
      label_dir = '/content/drive/MyDrive/Yelp_cities/{}_trainValidTest_{}/'.format(city, experiment)

    try:
      labels = pickle_load(label_dir + 'labels.pickle'.format(city))
    except:
      print('skippting:', city)
      continue
    
    # load testing data
    X_test = load_trainValidData(label_dir, city, x_only=True)

    # create the model, without TPU
    with strategy.scope():
      model = create_model(experiment=experiment, subExp=subExp, max_len=max_len)
    model.load_weights(load_dir+model_file_name)
    
    # Evaluate the model
    preds = model.predict(X_test)
    
    # Store the values in dicts
    predictions[city_origin] = preds.copy()

    del labels, X_test, model, preds
    gc.collect()
    

pickle_dump('{}predictions.pickle'.format(temp_result_dir), predictions)

del predictions
gc.collect()

"""#### 4 save business to categories dict"""

# 4. save business to categories dictionary
business_to_categories = {}

for city, model_file_name in model_mappings.items():
    print(city)
    city_origin = city
    if '_' in city:
      city, subExp = city.split('_')

    if experiment == '5n' or experiment == 'Synthetic':
      label_dir = '/content/drive/MyDrive/Yelp_cities/{}_trainValidTest_5f/'.format(city)
    else:
      label_dir = '/content/drive/MyDrive/Yelp_cities/{}_trainValidTest_{}/'.format(city, experiment)
    labels = pickle_load(label_dir + 'labels.pickle'.format(city))

    # load data frame
    city_df = pd.read_csv('{}/{}_reviews.csv'.format(data_dir, city), lineterminator='\n')
    city_df.rename(columns={'text': 'review_text'}, inplace=True)

    city_df = city_df[['business_id', 'review_text', 'name', 'categories']]
    city_df.dropna(subset=['business_id', 'review_text', 'name', 'categories'], inplace=True)
    city_df = city_df[city_df['business_id'].isin(labels)]
    city_df = city_df.sample(frac=1, random_state=1).reset_index(drop=True)

    bus_to_cat = get_business_to_categories(city_df)
    business_to_categories[city_origin] = bus_to_cat.copy()
    
    del city_df, bus_to_cat
    gc.collect()

pickle_dump('{}business_to_categories.pickle'.format(temp_result_dir), business_to_categories)

del business_to_categories
gc.collect()

"""## Helper functions"""

# Prepare the qrels
def get_qrels(y_test, labels, business_to_categories):
    qrels= {}
    with tqdm(total=len(y_test)) as pbar:
        for i in range(len(y_test)):
            r = {}
            business_id = labels[y_test[i]]
            categories1 = business_to_categories[business_id]

            for business_id in labels:
                categories2 = business_to_categories[business_id]
                if len(categories1.intersection(categories2)) != 0:
                    r[business_id] = int(1)
            qrels['r'+str(i)] = r
            pbar.update(1)    
    return qrels

# Prepare the results
def get_run(preds, labels):
    run = {}
    with tqdm(total=len(preds)) as pbar:
        for i in range(len(preds)):
            rec = preds[i]  
            r = {}
            for j in range(len(rec)):
                p = rec[j]
                business_id = labels[j]
                r[business_id] = float(p)
            run['r'+str(i)] = r            

            pbar.update(1)
    return run

def evaluate(qrels, run):
    s = 0
    res = {}

    run_items = list(run.items())
    run_items.sort(key=lambda x: x[0])
    qrels_items = list(qrels.items())
    qrels_items.sort(key=lambda x: x[0])

    for i in range(30):
        s = i * int(len(run)/30)
        e = int(len(run)/30) + s
        run_ = dict(run_items[s:e])
        qrels_ = dict(qrels_items[s:e])
        evaluator = pytrec_eval.RelevanceEvaluator(qrels_, {'map','P', 'recall', 'ndcg', 'recip_rank', 'Rprec',
                                                      })
        res.update(evaluator.evaluate(run_))
    out = {}
    for k in res:
        for m in res[k]:
            l = out.get(m, list())
            l.append(res[k][m])
            out[m] = l
    for k in out:
        if k not in ['num_q','num_rel','num_rel_ret','num_ret']: 
            out[k] = (np.mean(out[k]),(1.96 * np.std(out[k]))/np.sqrt(len(out[k])))
        else:
            out[k] = np.sum(out[k])
    return out

import warnings
from sklearn import metrics
import numpy as np
from sklearn.utils._encode import _unique, _encode

def computeMRR(y_true, y_predict):
  num_sample = len(y_true)
  r_rank = []
  for i in range(num_sample):
    target=y_true[i][0]
    pred = y_predict[i]
    if target not in pred:
      r_rank.append(0)
    else:
      r_rank.append(1 /(np.where(pred==target)[0] + 1))
  
  return np.mean(r_rank)

def top_k_accuracy_score(y_true, y_score, k=2):
    classes = [x for x in range(len(y_score[0]))]
    y_true_encoded = _encode(y_true, uniques=classes)
    sorted_pred = np.argsort(y_score, axis=1, kind="mergesort")[:, ::-1]
    hits = (y_true_encoded == sorted_pred[:, :k].T).any(axis=0)
    return np.mean(hits), (1.96 * np.std(hits)) / np.sqrt(len(hits))

def get_performance(y_true: list, y_pred: list):
    precision_score = metrics.precision_score(y_true, np.argmax(y_pred, axis=1), average='weighted')
    recall_score = metrics.recall_score(y_true, np.argmax(y_pred, axis=1), average='weighted')
    f1_score = metrics.f1_score(y_true, np.argmax(y_pred, axis=1), average='weighted')
    hit_rates5 = top_k_accuracy_score(y_true, y_pred, k=5)
    hit_rates10 = top_k_accuracy_score(y_true, y_pred, k=10)
    hit_rates20 = top_k_accuracy_score(y_true, y_pred, k=20)
    accuracy = top_k_accuracy_score(y_true, y_pred, k=1)
    mrr = computeMRR(np.array([[x] for x in y_true]), np.argsort(-1 * y_pred, axis=1))
    out = {'Precision': precision_score, 'Recall': recall_score, 'F1-Score': f1_score, 'Accuracy': accuracy,
           'MRR': mrr, 'HR@5': hit_rates5, 'HR@10': hit_rates10, 'HR@20': hit_rates20}
    return out

"""## Get model performance"""

experiment = 'Synthetic'
subExp = None #'freezeBoth'
# create saving directory
if experiment == '5n':
  temp_result_dir = '/content/drive/MyDrive/TF_TPU/Performance_Analysis/{}/{}/'.format(experiment, subExp)
else:
  temp_result_dir = '/content/drive/MyDrive/TF_TPU/Performance_Analysis/{}/'.format(experiment)
  
if not os.path.isdir(temp_result_dir):
  os.mkdir(temp_result_dir)

# Load saved data and get model performance 
with open('{}ground_truth.pickle'.format(temp_result_dir), 'rb') as file:
        ground_truth = pickle.load(file)
with open('{}predictions.pickle'.format(temp_result_dir), 'rb') as file:
        predictions = pickle.load(file)
with open('{}labels_dict.pickle'.format(temp_result_dir), 'rb') as file:
        labels_dict = pickle.load(file)

performance = {}
for city in model_mappings.keys():
    print(city)
    
    if city in ground_truth.keys():
      y_true = ground_truth[city]
      y_pred = predictions[city]
      performance[city] = get_performance(y_true, y_pred)
      
      del y_true, y_pred 
      gc.collect()
    else:
      print('Skipping city:', city)
    
with open('{}model_performance.pickle'.format(temp_result_dir), 'wb') as file:
  pickle.dump(performance, file)

"""## Get category performance

We save results for each city first
"""

with open('{}business_to_categories.pickle'.format(temp_result_dir), 'rb') as file:
        business_to_categories = pickle.load(file)

performance_cat = {}
for city in model_mappings.keys():
    print(city)
    qrels  = get_qrels(ground_truth[city], labels_dict[city], business_to_categories[city])
    run = get_run(predictions[city], labels_dict[city])
    performance_cat[city] = evaluate(qrels, run)

    del qrels, run
    gc.collect()

with open('{}performance_cat'.format(temp_result_dir)+'.pickle', 'wb') as file:
        pickle.dump(performance_cat, file)

# Delete data sources
del ground_truth, predictions, labels_dict, business_to_categories
gc.collect()

"""## Gather performance"""

experiment = '5f'
subExp = None #'freezeBoth'
if experiment == '5n':
  temp_result_dir = '/content/drive/MyDrive/TF_TPU/Performance_Analysis/{}/{}/'.format(experiment, subExp)
else:
  temp_result_dir = '/content/drive/MyDrive/TF_TPU/Performance_Analysis/{}/'.format(experiment)
if not os.path.isdir(temp_result_dir):
  os.mkdir(temp_result_dir)
df_columns = ['City','Prec', 'Recall', 'F1-Score', 'MRR', 'Accuracy', 'HR@5', 
              'HR@10', 'HR@20','Prec@5','Prec@10','Prec@20', 'R-Prec','MAP', 
              'MRR', 'nDCG']

with open('{}model_performance.pickle'.format(temp_result_dir), 'rb') as file:
        performance = pickle.load(file)
with open('{}performance_cat.pickle'.format(temp_result_dir), 'rb') as file:
        performance_cat = pickle.load(file)

data = pd.DataFrame(columns=df_columns)
r = 3
for city in performance.keys():
    perf = performance[city]
    perf_cat = performance_cat[city]
    data = data.append(pd.DataFrame([[city,
                                      round(perf['Precision'], r),
                                      round(perf['Recall'], r),
                                      round(perf['F1-Score'], r),
                                      round(perf['MRR'], r),
                                      round(perf['Accuracy'][0], r),
                                      round(perf['HR@5'][0], r),
                                      round(perf['HR@10'][0], r),
                                      round(perf['HR@20'][0], r),
                                      round(perf_cat['P_5'][0], r),
                                      round(perf_cat['P_10'][0], r),
                                      round(perf_cat['P_20'][0], r),
                                      round(perf_cat['Rprec'][0], r),
                                      round(perf_cat['map'][0], r),
                                      round(perf_cat['recip_rank'][0], r),
                                      round(perf_cat['ndcg'][0], r)]],
                                    columns=df_columns),ignore_index=True)

averages = [[],[]]
for i in range(1, data.shape[1]):
    array = list(data.iloc[:,i]) 
    ci = round(1.96 * np.std(array)/np.sqrt(len(array)), r)
    mean = round(np.mean(array), r)
    averages[0].append(mean)
    averages[1].append(ci)

averages[0].insert(0, 'Average')
averages[1].insert(0, '95% CI ±')
data = data.append(pd.DataFrame([averages[0]],
                                    columns=df_columns),ignore_index=True)
data = data.append(pd.DataFrame([averages[1]],
                                    columns=df_columns),ignore_index=True)

# Saving data
data.to_csv(temp_result_dir+'performance_table.csv', index=False)

# 5n freezeBoth
data

# 5n Toronto
data

# 4f
data

# 5f
data

"""## Gather Big Table performance

We gather the experiment results in one big table in this section
The deviation score table needs to get from the `Bias_Analysis` file

"""

import pandas as pd

from google.colab import drive
drive.mount('/content/drive')

experiment = '5n'
subExp = 'freezeBoth'
if experiment == '5n':
  temp_result_dir = '/content/drive/MyDrive/TF_TPU/Performance_Analysis/{}/{}/'.format(experiment, subExp)
else:
  temp_result_dir = '/content/drive/MyDrive/TF_TPU/Performance_Analysis/{}/'.format(experiment)

model_performance_metrics = ['MRR', 'HR@5', 'HR@10']
performance_table = pd.read_csv(temp_result_dir + 'performance_table.csv')
# performance_table = pd.read_csv(temp_result_dir + 'performance_table.csv')
sub_performance_table = performance_table[['City'] + model_performance_metrics]
sub_performance_table.set_index('City', inplace=True)

raceDeviationScore_table = pd.read_csv(temp_result_dir + 'performance_deviationScore_race.csv')
# raceDeviationScore_table = pd.read_csv(temp_result_dir + 'performance_deviationScore_race.csv')
raceDeviationScore_table.loc[raceDeviationScore_table['City'] == 'Overall', 'City'] = 'Average'
raceDeviationScore_table.columns = ['City'] + [col_name + ' (Race)' for col_name in raceDeviationScore_table.columns if col_name != 'City']
raceDeviationScore_table.set_index('City', inplace=True)

genderDeviationScore_table = pd.read_csv(temp_result_dir + 'performance_deviationScore_gender.csv')
# genderDeviationScore_table = pd.read_csv(temp_result_dir + 'performance_deviationScore_gender.csv')
genderDeviationScore_table.loc[genderDeviationScore_table['City'] == 'Overall', 'City'] = 'Average'
genderDeviationScore_table.columns = ['City'] + [col_name + ' (Gender)' for col_name in genderDeviationScore_table.columns if col_name != 'City']
genderDeviationScore_table.set_index('City', inplace=True)

# Innerjoin the tables
joinedDf = pd.concat([sub_performance_table, raceDeviationScore_table, genderDeviationScore_table], axis=1, join="inner")

# joinedDf.to_csv(temp_result_dir + 'performance_joinedDf.csv')
joinedDf.to_csv(temp_result_dir + 'performance_joinedDf.csv')

# 5n - freezeBoth
joinedDf

# 5n - Toronto
joinedDf

# 4f
joinedDf

# 5f
joinedDf

"""Append 5f Toronto and 5n Toronto together"""

temp_df = pd.read_csv('/content/drive/MyDrive/TF_TPU/Performance_Analysis/5f/performance_joinedDf.csv')

temp_df_toronto = temp_df.loc[temp_df['City'] == 'Toronto']
temp_df_toronto.set_index('City', inplace=True)

analyse_df = joinedDf.append(temp_df_toronto).reset_index()

analyse_df.loc[analyse_df['City'] != 'Average'].sort_values('MRR', ascending=False)

"""# Gather experiment performance comparison"""

comparison_df = None
experiment_list = ['5f', '4f', '5n_freezeBoth']
for experiment in experiment_list:
  print(experiment)
  subExp = None
  if '_' in experiment:
    experiment, subExp = experiment.split('_')

  if experiment == '5n':
    temp_dir = '/content/drive/MyDrive/TF_TPU/Performance_Analysis/{}/{}/'.format(experiment, subExp)
  else:
    temp_dir = '/content/drive/MyDrive/TF_TPU/Performance_Analysis/{}/'.format(experiment)

  print(temp_dir)
  # load performance table
  temp_perform_df = pd.read_csv('{}performance_joinedDf.csv'.format(temp_dir))
  temp_perform_df = temp_perform_df[temp_perform_df['City'] == 'Average'].drop('City', axis=1)
  if subExp is None:
    temp_perform_df['Experiment'] = experiment
  else:
    temp_perform_df['Experiment'] = experiment + '\n (' + subExp + ')'
  temp_cols = temp_perform_df.columns.to_list()
  temp_perform_df = temp_perform_df[temp_cols[-1:] + temp_cols[:-1]]

  if comparison_df is None:
    comparison_df = pd.DataFrame(columns = temp_perform_df.columns.to_list())

  comparison_df = comparison_df.append(temp_perform_df)


# comparison_df_cols = comparison_df.columns.to_list()
# for idx, col in enumerate(comparison_df_cols):
#   if '(Race)' in col:
#     comparison_df_cols[idx] = col.split(' (Race)')[0] + ' \n (Race)'
#   elif '(Gender)' in col:
#     comparison_df_cols[idx] = col.split(' (Gender)')[0] + ' \n (Gender)'
# comparison_df.columns = comparison_df_cols

comparison_df_origin = comparison_df.copy()

comparison_df = comparison_df_origin.copy()
comparison_percentage_df = comparison_df_origin.copy()

benchmark = '5f'
comparison_percentage_df = comparison_percentage_df[comparison_percentage_df['Experiment'] != benchmark]

comparison_cols = comparison_df.columns.to_list()
comparison_cols.remove('Experiment')


for metric_name in comparison_cols:
  subResults = comparison_df[comparison_df['Experiment'] != benchmark][metric_name]
  temp_benchMark = comparison_df.loc[comparison_df['Experiment'] == benchmark][metric_name].values[0]
  
  # Get percentage change
  temp_diff = round((100 * (subResults - temp_benchMark) / temp_benchMark), 2)
  comparison_percentage_df[metric_name] = temp_diff

  # Get overview 
  temp_diff = '\n (' + round((100 * (subResults - temp_benchMark) / temp_benchMark), 2).astype('str') + '%)' 
  # comparison_df[comparison_df['Experiment'] != benchmark][metric_name] = subResults.astype('str') + temp_diff
  comparison_df.loc[comparison_df.Experiment!=benchmark, metric_name] = subResults.astype('str') + temp_diff

comparison_df.reset_index(inplace=True, drop=True)
comparison_percentage_df.set_index('Experiment', inplace=True)

comparison_df

"""### Table overview"""

from IPython.display import display, HTML

display( HTML(comparison_df.to_html().replace("\\n","<br>") ) )

"""### Percentage overview"""

def style_negative(v, props=''):
    return props if v < 0 else None
def highlight_max(s, props=''):
    return np.where(s == np.nanmax(s.values), props, '')
def highlight_min(s, props=''):
    return np.where(s == np.nanmin(s.values), props, '')

s2 = comparison_percentage_df.style.applymap(style_negative, props='color:red;').applymap(lambda v: 'color:white;background-color:purple' if (v > 0) else None)
s2.apply(highlight_min, props='color:red;background-color:#ffffb3;', axis=0)

"""# (Redundant & Original) Performance Evaluation

Using the evaluation method provided by Reda's code

Evaluation for all city's data
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install pytrec_eval

# Commented out IPython magic to ensure Python compatibility.
import tensorflow as tf
from tensorflow import keras
import matplotlib.pyplot as plt
import pandas as pd
import pytrec_eval
import numpy as np
import seaborn as sns
from tqdm import tqdm
import pickle

# %matplotlib inline
# %load_ext autoreload
# %autoreload 2
# %load_ext tensorboard

def load_trainValidData(city_name):
  load_dir = '/content/drive/MyDrive/Yelp_cities/{}_Synthetic_trainValidTest/'.format(city_name)

  input_ids_list = pickle_load(load_dir+ 'input_ids_list.pickle'.format(city_name))
  attention_mask_list = pickle_load(load_dir + 'attention_mask_list.pickle'.format(city_name))
  y = pickle_load(load_dir + 'y.pickle'.format(city_name))
  labels = pickle_load(load_dir + 'labels.pickle'.format(city_name))

  # Split into train and test
  train_size = int(len(y) * 0.8)
  eval_size = int(len(y) * 0.1)

  # X_train = [np.array(input_ids_list[0:train_size]), np.array(attention_mask_list[0:train_size])]
  # y_train = np.array(y[0:train_size])
  # X_eval = [np.array(input_ids_list[train_size:train_size+eval_size]), np.array(attention_mask_list[train_size:train_size+eval_size])]
  # y_eval = np.array(y[train_size:train_size+eval_size])

  X_test = [np.array(input_ids_list[train_size+eval_size:]), np.array(attention_mask_list[train_size+eval_size:])]
  y_test = np.array(y[train_size+eval_size:])

  return X_test, y_test

def get_business_to_categories(df):
  business_to_categories = {}
  for business_id, categories in zip(df['business_id'], df['categories']):
      categories = multireplace(categories)
      categories = set(categories.lower().split(', '))
      if 'restaurants' in categories:
          categories.remove('restaurants')
      if business_id in business_to_categories:
          categories = categories | business_to_categories[business_id]
      business_to_categories[business_id] = categories
  return business_to_categories

"""## Predictions"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# # Create distribution strategy
# tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
# tf.config.experimental_connect_to_cluster(tpu)
# tf.tpu.experimental.initialize_tpu_system(tpu)
# strategy = tf.distribute.TPUStrategy(tpu)

model_mappings = {
    'Atlanta': 'bs_32model.05-0.2036.h5',
    'Austin':'bs_32model.07-0.1525.h5',
    'Boston': 'model.05-0.19.h5',
    'Columbus': 'model.06-0.19.h5',
    'Orlando': 'bs_64model.06-0.1701.h5',
    'Portland': 'bs_32model.05-0.1635.h5',
    'Toronto': 'bs_32model.05-0.19.h5'
}

predictions = {}
ground_truth = {}
business_to_categories = {}
labels_dict = {}

#change
for city, model_file_name in model_mappings.items():
    print(city)
    # Load dataset related items
    load_dir = '/content/drive/MyDrive/Research_material/LMRec/TF_TPU/models/yelp_{}_Synthetic/'.format(city)
    # load labels

    # shuffle
    labels = pickle_load(load_dir + 'labels.pickle'.format(city))
    num_labels = len(labels)

    # load data frame
    city_df = pd.read_csv('{}/{}_reviews.csv'.format(data_dir, city), lineterminator='\n')
    city_df.rename(columns={'text': 'review_text'}, inplace=True)

    city_df = city_df[['business_id', 'review_text', 'name', 'categories']]
    city_df.dropna(subset=['business_id', 'review_text', 'name', 'categories'], inplace=True)
    city_df = city_df[city_df['business_id'].isin(labels)]
    city_df = city_df.sample(frac=1, random_state=1).reset_index(drop=True)

    bus_to_cat = get_business_to_categories(city_df)

    # load testing data
    X_test, y_test = load_trainValidData(city)

    # create the model, without TPU
    with strategy.scope():
      model = create_model()
    model.load_weights(load_dir+model_file_name)
    
    # Evaluate the model
    preds = model.predict(X_test, verbose=1, batch_size=128)
    
    # Store the values in dicts
    predictions[city] = preds
    ground_truth[city] = y_test
    business_to_categories[city] = bus_to_cat
    labels_dict[city] = labels
    del city_df

pickle_dump('pickles/predictions.pickle', predictions)
pickle_dump('pickles/ground_truth.pickle', ground_truth)
pickle_dump('pickles/business_to_categories.pickle', business_to_categories)
pickle_dump('pickles/labels_dict.pickle', labels_dict)

"""### Helper functions"""

# Prepare the qrels
def get_qrels(y_test, labels, business_to_categories):
    qrels= {}
    with tqdm(total=len(y_test)) as pbar:
        for i in range(len(y_test)):
            r = {}
            business_id = labels[y_test[i]]
            categories1 = business_to_categories[business_id]

            for business_id in labels:
                categories2 = business_to_categories[business_id]
                if len(categories1.intersection(categories2)) != 0:
                    r[business_id] = int(1)
            qrels['r'+str(i)] = r
            pbar.update(1)    
    return qrels

# Prepare the results
def get_run(preds, labels):
    run = {}
    with tqdm(total=len(preds)) as pbar:
        for i in range(len(preds)):
            rec = preds[i]  
            r = {}
            for j in range(len(rec)):
                p = rec[j]
                business_id = labels[j]
                r[business_id] = float(p)
            run['r'+str(i)] = r            

            pbar.update(1)
    return run

def evaluate(qrels, run):
    s = 0
    res = {}

    run_items = list(run.items())
    run_items.sort(key=lambda x: x[0])
    qrels_items = list(qrels.items())
    qrels_items.sort(key=lambda x: x[0])

    for i in range(30):
        s = i * int(len(run)/30)
        e = int(len(run)/30) + s
        run_ = dict(run_items[s:e])
        qrels_ = dict(qrels_items[s:e])
        evaluator = pytrec_eval.RelevanceEvaluator(qrels_, {'map','P', 'recall', 'ndcg', 'recip_rank', 'Rprec',
                                                      })
        res.update(evaluator.evaluate(run_))
    out = {}
    for k in res:
        for m in res[k]:
            l = out.get(m, list())
            l.append(res[k][m])
            out[m] = l
    for k in out:
        if k not in ['num_q','num_rel','num_rel_ret','num_ret']: 
            out[k] = (np.mean(out[k]),(1.96 * np.std(out[k]))/np.sqrt(len(out[k])))
        else:
            out[k] = np.sum(out[k])
    return out

import warnings
from sklearn import metrics
import numpy as np
# import metrics_eval
from sklearn.utils._encode import _unique, _encode


def top_k_accuracy_score(y_true, y_score, k=2):
    classes = [x for x in range(len(y_score[0]))]
    y_true_encoded = _encode(y_true, uniques=classes)
    sorted_pred = np.argsort(y_score, axis=1, kind="mergesort")[:, ::-1]
    hits = (y_true_encoded == sorted_pred[:, :k].T).any(axis=0)
    return np.mean(hits), (1.96 * np.std(hits)) / np.sqrt(len(hits))

def get_performance(y_true: list, y_pred: list):
    precision_score = metrics.precision_score(y_true, np.argmax(y_pred, axis=1), average='weighted')
    recall_score = metrics.recall_score(y_true, np.argmax(y_pred, axis=1), average='weighted')
    f1_score = metrics.f1_score(y_true, np.argmax(y_pred, axis=1), average='weighted')
    hit_rates5 = top_k_accuracy_score(y_true, y_pred, k=5)
    hit_rates10 = top_k_accuracy_score(y_true, y_pred, k=10)
    hit_rates20 = top_k_accuracy_score(y_true, y_pred, k=20)
    accuracy = top_k_accuracy_score(y_true, y_pred, k=1)
    # mrr = metrics_eval.mrr(np.array([[x] for x in y_true]), np.argsort(-1 * y_pred, axis=1))
    out = {'Precision': precision_score, 'Recall': recall_score, 'F1-Score': f1_score, 'Accuracy': accuracy,
          #  'MRR': mrr,
           'HR@5': hit_rates5, 'HR@10': hit_rates10, 'HR@20': hit_rates20}
    return out

performance = {}
performance_cat = {}

if not os.path.exists('pickles'):
  os.mkdir('pickles')

for city in model_mappings.keys():
    print(city)
    
    # y_true = ground_truth[city]
    # y_pred = predictions[city]
    # performance[city] = get_performance(y_true, y_pred)

    qrels  = get_qrels(ground_truth[city], labels_dict[city], business_to_categories[city])
    run = get_run(predictions[city], labels_dict[city])
    performance_cat[city] = evaluate(qrels, run)

    with open('pickles/performance_cat_'+city+'.pickle', 'wb') as file:
        pickle.dump(performance_cat, file)
    
    # with open('pickles/performance.pickle', 'wb') as file:
      # pickle.dump(performance, file)

performance_cat = {}
for city in predictions.keys():
    print(city)
    with open('pickles/performance_cat_'+city+'.pickle', 'rb') as file:
        p = pickle.load(file)
    performance_cat[city] = p[city]

with open('pickles/performance.pickle', 'rb') as file:
        performance = pickle.load(file)

data = pd.DataFrame(columns=['City','Prec', 'Recall', 'F1-Score', 
                            #  'MRR', 
                             'Accuracy', 
                             'HR@5', 'HR@10', 'HR@20',
                             'Prec@5','Prec@10','Prec@20', 'R-Prec','MAP', 
                             'MRR', 
                             'nDCG'])
r = 3
for city in performance.keys():
    perf = performance[city]
    perf_cat = performance_cat[city]
    data = data.append(pd.DataFrame([[city.title(),
                                      round(perf['Precision'], r),
                                      round(perf['Recall'], r),
                                      round(perf['F1-Score'], r),
                                      # round(perf['MRR'], r),
                                      round(perf['Accuracy'][0], r),
                                      round(perf['HR@5'][0], r),
                                      round(perf['HR@10'][0], r),
                                      round(perf['HR@20'][0], r),
                                      round(perf_cat['P_5'][0], r),
                                      round(perf_cat['P_10'][0], r),
                                      round(perf_cat['P_20'][0], r),
                                      round(perf_cat['Rprec'][0], r),
                                      round(perf_cat['map'][0], r),
                                      round(perf_cat['recip_rank'][0], r),
                                      round(perf_cat['ndcg'][0], r)]],
                                    columns=['City','Prec', 'Recall', 'F1-Score', 
                                            #  'MRR', 
                                             'Accuracy', 
                                             'HR@5', 'HR@10', 'HR@20',
                                             'Prec@5','Prec@10','Prec@20', 'R-Prec','MAP', 
                                             'MRR', 
                                             'nDCG']),ignore_index=True)

averages = [[],[]]
for i in range(1, data.shape[1]):
    array = list(data.iloc[:,i]) 
    ci = round(1.96 * np.std(array)/np.sqrt(len(array)), r)
    mean = round(np.mean(array), r)
    averages[0].append(mean)
    averages[1].append(ci)

averages[0].insert(0, 'Average')
averages[1].insert(0, '95% CI ±')
data = data.append(pd.DataFrame([averages[0]],
                                    columns=['City','Prec', 'Recall', 'F1-Score', 
                                            #  'MRR', 
                                             'Accuracy', 
                                             'HR@5', 'HR@10', 'HR@20',
                                             'Prec@5','Prec@10','Prec@20', 'R-Prec','MAP', 'MRR', 'nDCG']),ignore_index=True)
data = data.append(pd.DataFrame([averages[1]],
                                    columns=['City','Prec', 'Recall', 'F1-Score', 
                                            #  'MRR', 
                                             'Accuracy', 
                                             'HR@5', 'HR@10', 'HR@20',
                                             'Prec@5','Prec@10','Prec@20', 'R-Prec','MAP', 'MRR', 'nDCG']),ignore_index=True)

data

print(data.to_latex(index=False))

