# -*- coding: utf-8 -*-
"""lmrec_Reda.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rlpUAN1OJeBrcZs01UwIJOpYErHmGAWd
"""

!pip install tokenizers transformers

import os
import re
import json
import string
import numpy as np
import pandas as pd
import statistics
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tokenizers import BertWordPieceTokenizer
from transformers import BertTokenizer, TFBertModel, BertConfig
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.utils import shuffle
from tensorflow.keras.utils import plot_model
configuration = BertConfig()  # default parameters and configuration for BERT

from google.colab import drive
drive.mount('/content/gdrive')

filename = "/content/gdrive/MyDrive/Research_material/Research_data/Cleaned_Toronto_Reviews.json"

# plots
def learning_plots(history):
    plt.figure(figsize=(15,4))
    ax1 = plt.subplot(1, 2, 1)
    for l in history.history:
        if l == 'loss' or l == 'val_loss':
            loss = history.history[l]
            plt.plot(range(1, len(loss) + 1), loss, label=l)

    plt.title('Training and validation loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    ax2 = plt.subplot(1, 2, 2)
    for k in history.history:
        if 'accuracy' in k:
            loss = history.history[k]
            plt.plot(range(1, len(loss) + 1), loss, label=k)
    plt.title('Training and validation accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()
    plt.savefig('learning.pdf', bbox_inches = 'tight')

# Save the slow pretrained tokenizer
slow_tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
save_path = "bert_base_uncased/"
if not os.path.exists(save_path):
    os.makedirs(save_path)
slow_tokenizer.save_pretrained(save_path)

# Load the fast tokenizer from saved file
tokenizer = BertWordPieceTokenizer("bert_base_uncased/vocab.txt", lowercase=True)

with open(filename, 'r') as f:
    data = f.readlines()
    data = list(map(json.loads, data))

data = data[0]

df = pd.DataFrame(data)
df.rename(columns={'stars': 'review_stars', 'text': 'review_text'},
          inplace=True)

df = df[['business_id', 'review_text']]
data = df.values.tolist()
df.head()

# count number of reviews for this dataset
business_id_dist = {}
with tqdm(total=len(data)) as pbar:
    for v in data:
        pbar.update(1)
        business_id = v[0]
        # Process text
        if business_id in business_id_dist:
          v = business_id_dist[business_id]
          business_id_dist[business_id] = v + 1
        else:
          business_id_dist[business_id] = 1

# plt.figure(figsize=(18,8))
# sns.histplot(business_id_dist.values(), kde=True)
# plt.xlabel('#businesses')
# plt.ylabel('#of reviews')

print('average number of reviews for each business:', statistics.mean(business_id_dist.values()))
print('median number of reviews for each business:', statistics.median(business_id_dist.values()))

min_reviews = 100

review_size_dist = []
with tqdm(total=len(data)) as pbar:
    for v in data:
        pbar.update(1)
        review_text = v[1]
        # Process text
        tokenized_review = tokenizer.encode(review_text)
        input_ids = tokenized_review.ids
        review_size_dist.append(len(input_ids))

plt.figure(figsize=(18,8))
sns.histplot(review_size_dist, kde=True)
plt.xlabel('Length of review')
plt.ylabel('#of reviews')

# maximum length of the review text
max_len = 400

labels = []
input_ids_list = []
attention_mask_list = []
y = []
with tqdm(total=len(data)) as pbar:
  for v in data:
      pbar.update(1)
      business_id = v[0]
      review_text = v[1]
      # check if enough review for the business
      if business_id_dist[business_id] < min_reviews:
        continue
      # Process text
      tokenized_review = tokenizer.encode(review_text)
      input_ids = tokenized_review.ids
      attention_mask = [1] * len(input_ids)
      # Pad and create attention masks.
      # Skip if truncation is needed
      padding_length = max_len - len(input_ids)
      if padding_length > 0:  # pad
          input_ids = input_ids + ([0] * padding_length)
          attention_mask = attention_mask + ([0] * padding_length)
      else:
          continue
      input_ids_list.append(input_ids)
      attention_mask_list.append(attention_mask)
      # Process labels
      if business_id not in labels:
          labels.append(business_id)
      y.append(labels.index(business_id))

len(set(y))

# Split into train and test
train_size = int(len(y) * 0.8)
eval_size = int(len(y) * 0.1)

input_ids_list, attention_mask_list, y = shuffle(input_ids_list, attention_mask_list, y, random_state=0)
X_train = [np.array(input_ids_list[0:train_size]), np.array(attention_mask_list[0:train_size])]
y_train = np.array(y[0:train_size])

X_eval = [np.array(input_ids_list[train_size:train_size+eval_size]), np.array(attention_mask_list[train_size:train_size+eval_size])]
y_eval = np.array(y[train_size:train_size+eval_size])

X_test = [np.array(input_ids_list[train_size+eval_size:]), np.array(attention_mask_list[train_size+eval_size:])]
y_test = np.array(y[train_size+eval_size:])

def create_model():
    ## BERT encoder
    encoder = TFBertModel.from_pretrained("bert-base-uncased")

    ## Model
    input_ids = layers.Input(shape=(max_len,), dtype=tf.int32)
    attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32)
    embedding = encoder(
        input_ids, attention_mask=attention_mask
    )['pooler_output']

    dense = layers.Dense(1024, activation='relu')(embedding)
    out = layers.Dense(len(labels), activation='softmax')(dense)

    model = keras.Model(
        inputs=[input_ids, attention_mask],
        outputs=out,
    )
    loss = keras.losses.SparseCategoricalCrossentropy()
    optimizer = keras.optimizers.Adam(learning_rate=5e-5)
    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])
    return model

# Create distribution strategy
tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
tf.config.experimental_connect_to_cluster(tpu)
tf.tpu.experimental.initialize_tpu_system(tpu)
strategy = tf.distribute.TPUStrategy(tpu)

# Create model
with strategy.scope():
  model = create_model()

model.summary()
plot_model(model)

history = model.fit(
    X_train,
    y_train,
    validation_data = (X_eval, y_eval),
    epochs=10,
    verbose=1,
    batch_size=64,
)
learning_plots(history)

